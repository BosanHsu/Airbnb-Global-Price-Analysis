{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a18ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------------------+\n",
      "|          Amenity| count|      AveragePrice|\n",
      "+-----------------+------+------------------+\n",
      "|Wireless Internet|301897|  139.894383855269|\n",
      "|          Kitchen|296850|141.42693088455155|\n",
      "|          Heating|285280|138.53689722531155|\n",
      "|       Essentials|272512|140.51520093750932|\n",
      "|           Washer|235544|142.11130688508692|\n",
      "|               TV|225280|  154.405368044634|\n",
      "|         Internet|190759| 144.2520076196168|\n",
      "|          Hangers|183868| 138.5601776658159|\n",
      "|          Shampoo|183126| 141.3421657196129|\n",
      "|   Smoke detector|177721|147.89176864200925|\n",
      "+-----------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, avg\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AmenitiesWordCount\").getOrCreate()\n",
    "df = spark.read.csv('airbnb-listings.csv', header=True, inferSchema=True)\n",
    "\n",
    "# group by Amenity\n",
    "df_exploded = df.withColumn('Amenity', explode(split(col('Amenities'), ',')))\n",
    "\n",
    "# wordcount\n",
    "word_counts = df_exploded.groupBy('Amenity').count().orderBy(col('count').desc())\n",
    "\n",
    "# top 10 frequent word\n",
    "top_words = word_counts.limit(10)\n",
    "\n",
    "# top 10's avg price\n",
    "average_prices_ame = df_exploded.groupBy('Amenity').agg(avg(col('Price')).alias('AveragePrice'))\n",
    "\n",
    "\n",
    "top_words_with_price = top_words.join(average_prices_ame, 'Amenity').select('Amenity', 'count', 'AveragePrice')\n",
    "top_words_with_price.orderBy(col('count').desc()).limit(10).show()\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501fb763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+------------------+\n",
      "|        Transit|count|      AveragePrice|\n",
      "+---------------+-----+------------------+\n",
      "|          phone|13248| 9.730272202364587|\n",
      "|          email|13087| 9.733401240855635|\n",
      "|        reviews|12718| 9.731131643948403|\n",
      "|          jumio| 7965| 9.738031119090365|\n",
      "|  United States| 5892| 9.904449741756059|\n",
      "|            1.0| 5706| 9.570974576271187|\n",
      "| United Kingdom| 3762|  9.66472602739726|\n",
      "|          Spain| 3389| 9.617452440033086|\n",
      "|       facebook| 3288| 9.789454545454545|\n",
      "|         France| 3044|10.288540534253647|\n",
      "+---------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"AmenitiesWordCount\").getOrCreate()\n",
    "df = spark.read.csv('airbnb-listings.csv', header=True, inferSchema=True)\n",
    "\n",
    "# group by Transit\n",
    "df_exploded = df.withColumn('Transit', explode(split(col('Transit'), ',')))\n",
    "\n",
    "# wordcount\n",
    "word_counts = df_exploded.groupBy('Transit').count().orderBy(col('count').desc())\n",
    "\n",
    "# top 10 frequent word\n",
    "top_words = word_counts.limit(10)\n",
    "\n",
    "# top 10's avg price\n",
    "average_prices_transit = df_exploded.groupBy('Transit').agg(avg(col('Price')).alias('AveragePrice'))\n",
    "\n",
    "\n",
    "top_words_with_price = top_words.join(average_prices_transit, 'Transit').select('Transit', 'count', 'AveragePrice')\n",
    "top_words_with_price.orderBy(col('count').desc()).limit(10).show()\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18defc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+------------------------+\n",
      "|Host Response Time|Count_Number_of_Reviews|Avg_Review_Scores_Rating|\n",
      "+------------------+-----------------------+------------------------+\n",
      "|within a few hours|                   5144|       93.33547257876313|\n",
      "|    within an hour|                  18333|       92.95301757066463|\n",
      "|      within a day|                   2047|       92.86112469437653|\n",
      "|a few days or more|                    119|        89.8655462184874|\n",
      "+------------------+-----------------------+------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AirbnbDataAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"airbnb-listings.csv\")\n",
    "\n",
    "df = df.dropna(subset=[\"Host Response Time\"])\n",
    "# Cast the 'Number of Reviews' to Integer and 'Last Review' to Date\n",
    "df = df.withColumn(\"Number of Reviews\", df[\"Number of Reviews\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Last Review\", to_date(df[\"Last Review\"], \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"Review Scores Rating\", df[\"Review Scores Rating\"].cast(FloatType()))\n",
    "\n",
    "# Create a temporary view to run SQL queries\n",
    "df.createOrReplaceTempView(\"listings\")\n",
    "\n",
    "# Now run the SQL query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    `Host Response Time`,\n",
    "    COUNT(`Number of Reviews`) AS Count_Number_of_Reviews,\n",
    "    AVG(`Review Scores Rating`) AS Avg_Review_Scores_Rating\n",
    "FROM listings\n",
    "WHERE `Number of Reviews` > 50 AND `Last Review` > '2016-01-01'\n",
    "GROUP BY `Host Response Time`\n",
    "ORDER BY AVG(`Review Scores Rating`) DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(4)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "481a9245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------------+-----------------+------+\n",
      "|       City|   Neighbourhood|  Property Type|        AvgRating|COUNTS|\n",
      "+-----------+----------------+---------------+-----------------+------+\n",
      "|  Amsterdam|        Oud-West|      Apartment|94.33739130434783|  1150|\n",
      "|     Berlin|        Neukölln|      Apartment| 93.6020482809071|  1367|\n",
      "|   Brooklyn|    Williamsburg|      Apartment|93.89115646258503|  1617|\n",
      "|   Brooklyn|    Williamsburg|           Loft| 94.4014598540146|   137|\n",
      "|  København|        Nørrebro|      Apartment|94.19538572458544|  1387|\n",
      "|     London| LB of Islington|          House|92.35036496350365|   137|\n",
      "|     London| LB of Islington|      Apartment|92.46984924623115|   398|\n",
      "|Los Angeles|    Mid-Wilshire|      Apartment|92.56801195814649|   669|\n",
      "|Los Angeles|    Mid-Wilshire|          House|94.08243727598567|   279|\n",
      "|   New York| Upper West Side|      Apartment|93.18666666666667|   900|\n",
      "|      Paris|      Montmartre|      Apartment|92.30014124293785|  1416|\n",
      "|       Roma|           Prati|Bed & Breakfast|91.61578947368422|   190|\n",
      "|       Roma|           Prati|      Apartment|93.44505494505495|   546|\n",
      "|    Toronto|Downtown Toronto|          House|             92.3|   100|\n",
      "|    Toronto|Downtown Toronto|      Apartment|93.54385964912281|   456|\n",
      "|    Toronto|Downtown Toronto|    Condominium|94.73451327433628|   226|\n",
      "+-----------+----------------+---------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TopCitiesNeighbourhoodAnalysis\").getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"airbnb-listings.csv\")\n",
    "\n",
    "\n",
    "df = df.na.drop(subset=[\"City\", \"Neighbourhood\", \"Property Type\", \"Review Scores Rating\"])\n",
    "\n",
    "df = df.withColumn(\"Review Scores Rating\", col(\"Review Scores Rating\").cast(\"float\"))\n",
    "\n",
    "df.createOrReplaceTempView(\"listings\")\n",
    "\n",
    "query = \"\"\"\n",
    "WITH CityFrequency AS (\n",
    "    SELECT City, COUNT(*) AS Listings\n",
    "    FROM listings\n",
    "    GROUP BY City\n",
    "    ORDER BY Listings DESC\n",
    "    LIMIT 10\n",
    "),\n",
    "NeighbourhoodFrequency AS (\n",
    "    SELECT City, Neighbourhood, COUNT(*) AS Listings\n",
    "    FROM listings\n",
    "    WHERE City IN (SELECT City FROM CityFrequency)\n",
    "    GROUP BY City, Neighbourhood\n",
    "),\n",
    "MaxNeighbourhoodPerCity AS (\n",
    "    SELECT City, MAX(Listings) AS MaxListings\n",
    "    FROM NeighbourhoodFrequency\n",
    "    GROUP BY City\n",
    "),\n",
    "TopNeighbourhoods AS (\n",
    "    SELECT nf.City, nf.Neighbourhood\n",
    "    FROM NeighbourhoodFrequency nf\n",
    "    INNER JOIN MaxNeighbourhoodPerCity mnc\n",
    "    ON nf.City = mnc.City AND nf.Listings = mnc.MaxListings\n",
    ")\n",
    "SELECT tn.City, tn.Neighbourhood, l.`Property Type`, AVG(l.`Review Scores Rating`) AS AvgRating, COUNT(tn.Neighbourhood) AS COUNTS\n",
    "FROM TopNeighbourhoods tn\n",
    "JOIN listings l ON tn.City = l.City AND tn.Neighbourhood = l.Neighbourhood\n",
    "GROUP BY tn.City, tn.Neighbourhood, l.`Property Type`\n",
    "HAVING COUNTS >= 100\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.orderBy(col('City').asc()).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9042a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
